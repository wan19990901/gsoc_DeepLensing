{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:1]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-01T02:26:19.528722Z","iopub.execute_input":"2023-04-01T02:26:19.529717Z","iopub.status.idle":"2023-04-01T02:26:33.910573Z","shell.execute_reply.started":"2023-04-01T02:26:19.529680Z","shell.execute_reply":"2023-04-01T02:26:33.909502Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/lensezip/lenses/no_sub/image_303470882091450624806213444518141820449.jpg\n/kaggle/input/lensezip/lenses/sub/image_189448974206662542797032450204033033.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoImageProcessor\n\ncheckpoint = \"google/vit-base-patch16-224-in21k\"\nimage_processor = AutoImageProcessor.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:26:33.912536Z","iopub.execute_input":"2023-04-01T02:26:33.912996Z","iopub.status.idle":"2023-04-01T02:26:45.270917Z","shell.execute_reply.started":"2023-04-01T02:26:33.912956Z","shell.execute_reply":"2023-04-01T02:26:45.269819Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b7e12138edc481183447fbe892fb868"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c3df17a151e4165a39e9df4c0a25ddd"}},"metadata":{}}]},{"cell_type":"code","source":"from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\nimport torch\nnormalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\nsize = (\n    image_processor.size[\"shortest_edge\"]\n    if \"shortest_edge\" in image_processor.size\n    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n)\n_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:26:45.272803Z","iopub.execute_input":"2023-04-01T02:26:45.273613Z","iopub.status.idle":"2023-04-01T02:26:45.496589Z","shell.execute_reply.started":"2023-04-01T02:26:45.273573Z","shell.execute_reply":"2023-04-01T02:26:45.495535Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def transforms(examples):\n    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n    del examples[\"image\"]\n    return examples","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:26:45.499518Z","iopub.execute_input":"2023-04-01T02:26:45.500292Z","iopub.status.idle":"2023-04-01T02:26:45.505975Z","shell.execute_reply.started":"2023-04-01T02:26:45.500251Z","shell.execute_reply":"2023-04-01T02:26:45.504950Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import Dataset\nfrom PIL import Image\n\nlabel_to_str = {0: 'no_sub', 1: 'sub'}\nstr_to_label = {'sub': 1, 'no_sub': 0}\n\ndef create_dataset(folder_path, train_ratio=0.9):\n    def get_label(file_path):\n        # Extract the label from the file path\n        parts = os.path.split(file_path)\n        if parts[-2].split('/')[-1] == \"sub\":\n            return 1\n        else:\n            return 0\n\n    # Get the list of all image file paths\n    sub_dir = os.path.join(folder_path, \"sub\")\n    no_sub_dir = os.path.join(folder_path, \"no_sub\")\n\n    sub_files = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith(\".jpg\")]\n    no_sub_files = [os.path.join(no_sub_dir, f) for f in os.listdir(no_sub_dir) if f.endswith(\".jpg\")]\n\n    all_files = sub_files + no_sub_files\n\n    # Shuffle the file paths and split them into training and testing sets\n    np.random.shuffle(all_files)\n    split_idx = int(len(all_files) * train_ratio)\n    train_files = all_files[:split_idx]\n    test_files = all_files[split_idx:]\n\n    # Create a list of dictionaries containing file paths and their corresponding labels\n    train_data = [{\"file_path\": file_path, \"label\": get_label(file_path)} for file_path in train_files]\n    test_data = [{\"file_path\": file_path, \"label\": get_label(file_path)} for file_path in test_files]\n\n    # Convert the list of dictionaries to Hugging Face's Dataset objects\n    train_ds = Dataset.from_dict({\"file_path\": [item[\"file_path\"] for item in train_data],\n                                  \"label\": [item[\"label\"] for item in train_data],\n                                 \"image\": [Image.open(item[\"file_path\"]) for item in train_data]})\n    test_ds = Dataset.from_dict({\"file_path\": [item[\"file_path\"] for item in test_data],\n                                 \"label\": [item[\"label\"] for item in test_data],\n                                \"image\": [Image.open(item[\"file_path\"]) for item in test_data]})\n\n    return train_ds, test_ds\n\nfolder_path = \"/kaggle/input/lensezip/lenses\"\ntrain_ds, test_ds = create_dataset(folder_path)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:26:45.507464Z","iopub.execute_input":"2023-04-01T02:26:45.508030Z","iopub.status.idle":"2023-04-01T02:27:28.448626Z","shell.execute_reply.started":"2023-04-01T02:26:45.507992Z","shell.execute_reply":"2023-04-01T02:27:28.447541Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"label_counts = {}\nfor example in test_ds:\n    label = example[\"label\"]\n    if label in label_counts:\n        label_counts[label] += 1\n    else:\n        label_counts[label] = 1\n\n# Print the label counts\nprint(label_counts)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:27:28.450026Z","iopub.execute_input":"2023-04-01T02:27:28.450724Z","iopub.status.idle":"2023-04-01T02:27:29.258283Z","shell.execute_reply.started":"2023-04-01T02:27:28.450683Z","shell.execute_reply":"2023-04-01T02:27:29.257277Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{1: 496, 0: 504}\n","output_type":"stream"}]},{"cell_type":"code","source":"train_ds = train_ds.with_transform(transforms)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:27:29.260716Z","iopub.execute_input":"2023-04-01T02:27:29.261119Z","iopub.status.idle":"2023-04-01T02:27:29.268618Z","shell.execute_reply.started":"2023-04-01T02:27:29.261078Z","shell.execute_reply":"2023-04-01T02:27:29.266108Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test_ds = test_ds.with_transform(transforms)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:27:29.270192Z","iopub.execute_input":"2023-04-01T02:27:29.271346Z","iopub.status.idle":"2023-04-01T02:27:29.278487Z","shell.execute_reply.started":"2023-04-01T02:27:29.271284Z","shell.execute_reply":"2023-04-01T02:27:29.277389Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import DefaultDataCollator\nfrom scipy.special import softmax\ndata_collator = DefaultDataCollator()\nfrom datasets import load_metric\n\nmetric = load_metric(\"roc_auc\")\ndef compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n#     print(eval_pred.predictions)\n#     predictions = np.argmax(eval_pred.predictions, axis=1)\n\n    probabilities = softmax(eval_pred.predictions, axis=1)[:,1]\n    return metric.compute(prediction_scores=probabilities, references=eval_pred.label_ids)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:27:29.280157Z","iopub.execute_input":"2023-04-01T02:27:29.280662Z","iopub.status.idle":"2023-04-01T02:27:30.406249Z","shell.execute_reply.started":"2023-04-01T02:27:29.280601Z","shell.execute_reply":"2023-04-01T02:27:30.405350Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be4a438078ed47348f0da827415c256e"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\nid_dict = {0:'no_sub', 1:'sub'}\nstr_to_id = {'sub':1,'no_sub':0}\nmodel = AutoModelForImageClassification.from_pretrained(\n    checkpoint,\n    num_labels=2,\n    id2label=id_dict,\n    label2id=str_to_id,\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:27:30.409540Z","iopub.execute_input":"2023-04-01T02:27:30.409840Z","iopub.status.idle":"2023-04-01T02:27:34.851793Z","shell.execute_reply.started":"2023-04-01T02:27:30.409811Z","shell.execute_reply":"2023-04-01T02:27:34.850816Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"492175c1277b4e6b8f9a1a12afbc56f7"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"vit-base\",\n    remove_unused_columns=False,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=16,\n    num_train_epochs=20,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"roc_auc\",\n    push_to_hub=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:28:19.930842Z","iopub.execute_input":"2023-04-01T02:28:19.931838Z","iopub.status.idle":"2023-04-01T02:28:20.014010Z","shell.execute_reply.started":"2023-04-01T02:28:19.931752Z","shell.execute_reply":"2023-04-01T02:28:20.013064Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import EarlyStoppingCallback\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    tokenizer=image_processor,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Stop training if the model is not improving for 3 consecutive evaluations\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:29:23.311580Z","iopub.execute_input":"2023-04-01T02:29:23.312332Z","iopub.status.idle":"2023-04-01T02:29:27.576947Z","shell.execute_reply.started":"2023-04-01T02:29:23.312281Z","shell.execute_reply":"2023-04-01T02:29:27.575910Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:29:34.912410Z","iopub.execute_input":"2023-04-01T02:29:34.913100Z","iopub.status.idle":"2023-04-01T03:03:23.479247Z","shell.execute_reply.started":"2023-04-01T02:29:34.913058Z","shell.execute_reply":"2023-04-01T03:03:23.478281Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.14.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230401_022946-6izhx9tf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/wan19990901/huggingface/runs/6izhx9tf' target=\"_blank\">radiant-snow-38</a></strong> to <a href='https://wandb.ai/wan19990901/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/wan19990901/huggingface' target=\"_blank\">https://wandb.ai/wan19990901/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/wan19990901/huggingface/runs/6izhx9tf' target=\"_blank\">https://wandb.ai/wan19990901/huggingface/runs/6izhx9tf</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1407' max='2800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1407/2800 32:59 < 32:42, 0.71 it/s, Epoch 9/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Roc Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.515300</td>\n      <td>0.476675</td>\n      <td>0.882048</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.512000</td>\n      <td>0.458021</td>\n      <td>0.890265</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.449000</td>\n      <td>0.330150</td>\n      <td>0.935116</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.360600</td>\n      <td>0.436731</td>\n      <td>0.930236</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.297500</td>\n      <td>0.228494</td>\n      <td>0.968966</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.251500</td>\n      <td>0.240947</td>\n      <td>0.967926</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.244000</td>\n      <td>0.185650</td>\n      <td>0.978903</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.202100</td>\n      <td>0.253686</td>\n      <td>0.966778</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.187700</td>\n      <td>0.291563</td>\n      <td>0.965978</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.184500</td>\n      <td>0.212881</td>\n      <td>0.975734</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1407, training_loss=0.3433399657111856, metrics={'train_runtime': 2028.5297, 'train_samples_per_second': 88.734, 'train_steps_per_second': 1.38, 'total_flos': 6.97427906531328e+18, 'train_loss': 0.3433399657111856, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"best_model = trainer.model\nbest_metrics = trainer.evaluate(test_ds)\nprint(\"Best model ROC-AUC score on the test set:\", best_metrics[\"eval_roc_auc\"])\n\n# Save the best model\nbest_model.save_pretrained(\"vit_base\")\n\n# Load the best model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:08:53.813533Z","iopub.execute_input":"2023-04-01T03:08:53.813928Z","iopub.status.idle":"2023-04-01T03:09:03.341309Z","shell.execute_reply.started":"2023-04-01T03:08:53.813892Z","shell.execute_reply":"2023-04-01T03:09:03.339736Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Best model ROC-AUC score on the test set: 0.9775785650281618\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1063399954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFAutoModelForImageClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vit_base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'TFAutoModelForImageClassification' is not defined"],"ename":"NameError","evalue":"name 'TFAutoModelForImageClassification' is not defined","output_type":"error"}]},{"cell_type":"code","source":"loaded_model = AutoModelForImageClassification.from_pretrained(\"vit_base\")","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:10:17.318443Z","iopub.execute_input":"2023-04-01T03:10:17.318979Z","iopub.status.idle":"2023-04-01T03:10:18.474862Z","shell.execute_reply.started":"2023-04-01T03:10:17.318925Z","shell.execute_reply":"2023-04-01T03:10:18.473383Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink ","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:14:30.765740Z","iopub.execute_input":"2023-04-01T03:14:30.766438Z","iopub.status.idle":"2023-04-01T03:14:30.772714Z","shell.execute_reply.started":"2023-04-01T03:14:30.766403Z","shell.execute_reply":"2023-04-01T03:14:30.771493Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"!FileLink(\"vit_base\")","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:14:59.633266Z","iopub.execute_input":"2023-04-01T03:14:59.633858Z","iopub.status.idle":"2023-04-01T03:15:00.714217Z","shell.execute_reply.started":"2023-04-01T03:14:59.633804Z","shell.execute_reply":"2023-04-01T03:15:00.709551Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"/bin/bash: -c: line 0: syntax error near unexpected token `\"vit_base\"'\n/bin/bash: -c: line 0: `FileLink(\"vit_base\")'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Visulization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n# Extract the training and evaluation history\ntraining_history = trainer.state.log_history\n\n# Extract the training loss, evaluation loss, and evaluation ROC-AUC\ntrain_loss = [entry['loss'] for entry in training_history if 'loss' in entry]\neval_loss = [entry['eval_loss'] for entry in training_history if 'eval_loss' in entry]\neval_roc_auc = [entry['eval_roc_auc'] for entry in training_history if 'eval_roc_auc' in entry]\n\n# Plot the training loss\nplt.plot(train_loss, label=\"Training Loss\")\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss\")\nplt.legend()\nplt.show()\n\n# Plot the evaluation loss and ROC-AUC\nepochs = list(range(1, len(eval_loss) + 1))\nfig, ax1 = plt.subplots()\n\nax1.set_xlabel(\"Epochs\")\nax1.setylabel(\"Loss\")\nax1.plot(epochs, eval_loss, label=\"Evaluation Loss\", color='tab:red')\nax1.tick_params(axis='y', labelcolor='tab:red')\nax1.legend(loc='upper left')\n\nax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis\nax2.set_ylabel(\"ROC-AUC\")\nax2.plot(epochs, eval_roc_auc, label=\"Evaluation ROC-AUC\", color='tab:blue')\nax2.tick_params(axis='y', labelcolor='tab:blue')\nax2.legend(loc='upper right')\n\nfig.tight_layout() # otherwise the right y-label is slightly clipped\nplt.title(\"Evaluation Loss and ROC-AUC\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:03:33.144140Z","iopub.status.idle":"2023-04-01T03:03:33.144520Z","shell.execute_reply.started":"2023-04-01T03:03:33.144340Z","shell.execute_reply":"2023-04-01T03:03:33.144361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}